{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
    "    \"\"\"Performs spectral normalization on weights.\n",
    "    This wrapper controls the Lipschitz constant of the layer by\n",
    "    constraining its spectral norm, which can stabilize the training of GANs.\n",
    "    See [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957).\n",
    "    Wrap `tf.keras.layers.Conv2D`:\n",
    "    >>> x = np.random.rand(1, 10, 10, 1)\n",
    "    >>> conv2d = SpectralNormalization(tf.keras.layers.Conv2D(2, 2))\n",
    "    >>> y = conv2d(x)\n",
    "    >>> y.shape\n",
    "    TensorShape([1, 9, 9, 2])\n",
    "    Wrap `tf.keras.layers.Dense`:\n",
    "    >>> x = np.random.rand(1, 10, 10, 1)\n",
    "    >>> dense = SpectralNormalization(tf.keras.layers.Dense(10))\n",
    "    >>> y = dense(x)\n",
    "    >>> y.shape\n",
    "    TensorShape([1, 10, 10, 10])\n",
    "    Args:\n",
    "      layer: A `tf.keras.layers.Layer` instance that\n",
    "        has either `kernel` or `embeddings` attribute.\n",
    "      power_iterations: `int`, the number of iterations during normalization.\n",
    "    Raises:\n",
    "      AssertionError: If not initialized with a `Layer` instance.\n",
    "      ValueError: If initialized with negative `power_iterations`.\n",
    "      AttributeError: If `layer` does not has `kernel` or `embeddings` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer: tf.keras.layers, power_iterations: int = 1, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        if power_iterations <= 0:\n",
    "            raise ValueError(\n",
    "                \"`power_iterations` should be greater than zero, got \"\n",
    "                \"`power_iterations={}`\".format(power_iterations)\n",
    "            )\n",
    "        self.power_iterations = power_iterations\n",
    "        self._initialized = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build `Layer`\"\"\"\n",
    "        super().build(input_shape)\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])\n",
    "\n",
    "        if hasattr(self.layer, \"kernel\"):\n",
    "            self.w = self.layer.kernel\n",
    "        elif hasattr(self.layer, \"embeddings\"):\n",
    "            self.w = self.layer.embeddings\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"{} object has no attribute 'kernel' nor \"\n",
    "                \"'embeddings'\".format(type(self.layer).__name__)\n",
    "            )\n",
    "\n",
    "        self.w_shape = self.w.shape.as_list()\n",
    "\n",
    "        self.u = self.add_weight(\n",
    "            shape=(1, self.w_shape[-1]),\n",
    "            initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=False,\n",
    "            name=\"sn_u\",\n",
    "            dtype=self.w.dtype,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Call `Layer`\"\"\"\n",
    "        if training is None:\n",
    "            training = tf.keras.backend.learning_phase()\n",
    "\n",
    "        if training:\n",
    "            self.normalize_weights()\n",
    "\n",
    "        output = self.layer(inputs)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n",
    "\n",
    "    @tf.function\n",
    "    def normalize_weights(self):\n",
    "        \"\"\"Generate spectral normalized weights.\n",
    "        This method will update the value of `self.w` with the\n",
    "        spectral normalized value, so that the layer is ready for `call()`.\n",
    "        \"\"\"\n",
    "\n",
    "        w = tf.reshape(self.w, [-1, self.w_shape[-1]])\n",
    "        u = self.u\n",
    "\n",
    "        with tf.name_scope(\"spectral_normalize\"):\n",
    "            for _ in range(self.power_iterations):\n",
    "                v = tf.math.l2_normalize(tf.matmul(u, w, transpose_b=True))\n",
    "                u = tf.math.l2_normalize(tf.matmul(v, w))\n",
    "\n",
    "            sigma = tf.matmul(tf.matmul(v, w), u, transpose_b=True)\n",
    "\n",
    "            self.w.assign(self.w / sigma)\n",
    "            self.u.assign(u)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"power_iterations\": self.power_iterations}\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self,kernel_sizes = 3, **kwargs):\n",
    "        super(ResBlock, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.relU_1 = layers.ReLU()\n",
    "        self.bn_1 = layers.BatchNormalization()\n",
    "        self.conv2d_1 = layers.Conv2D(channels, 3, padding = 'same')\n",
    "        self.relU_2 = layers.ReLU()\n",
    "        self.bn_2 = layers.BatchNormalization()\n",
    "        self.conv2d_2 = layers.Conv2D(channels, 3, padding = 'same')\n",
    "    def call(self, inputs):\n",
    "        x = self.relU_1(inputs)\n",
    "        x = self.bn_1(inputs)\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.relU_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        out = inputs + x\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockSN(layers.Layer):\n",
    "    def __init__(self,kernel_sizes = 3, **kwargs):\n",
    "        super(ResBlockSN, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.relU_1 = layers.ReLU()\n",
    "        self.bn_1 = layers.BatchNormalization()\n",
    "        self.conv2d_1 = SpectralNormalization(layers.Conv2D(channels, 3, padding = 'same'))\n",
    "        self.relU_2 = layers.ReLU()\n",
    "        self.bn_2 = layers.BatchNormalization()\n",
    "        self.conv2d_2 = SpectralNormalization(layers.Conv2D(channels, 3, padding = 'same'))\n",
    "    def call(self, inputs):\n",
    "        x = self.relU_1(inputs)\n",
    "        x = self.bn_1(inputs)\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.relU_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        out = inputs + x\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockCN(layers.Layer):\n",
    "    def __init__(self,class_num,embedding_size, kernel_sizes = 3, **kwargs):\n",
    "        super(ResBlockCN, self).__init__(**kwargs)\n",
    "        self.class_num = class_num\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #assert (type(input_shape) == list)\n",
    "        \n",
    "        features, label = input_shape[0], input_shape[1]\n",
    "        channels = features[-1]\n",
    "        \n",
    "        self.relU_1 = layers.ReLU()\n",
    "        self.cbn_1 = ConditionalBatchNorm(self.class_num, embedding_size=self.embedding_size)\n",
    "        self.conv2d_1 = layers.Conv2D(channels, 3, padding = 'same')\n",
    "        self.relU_2 = layers.ReLU()\n",
    "        self.cbn_2 = ConditionalBatchNorm(self.class_num, embedding_size=self.embedding_size)\n",
    "        self.conv2d_2 = layers.Conv2D(channels, 3, padding = 'same')\n",
    "    def call(self, inputs):\n",
    "        features, label = inputs[0], inputs[1]\n",
    "        x = self.relU_1(features)\n",
    "        x = self.cbn_1([x,label])\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.relU_2(x)\n",
    "        x = self.cbn_2([x,label])\n",
    "        x = self.conv2d_2(x)\n",
    "        out = features + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = tf.random.uniform((4,8,8,3))\n",
    "test_labels = tf.ones((4,1))\n",
    "res_cbn = ResBlockCN(9,32)\n",
    "print(res_cbn([test_tensor,test_labels]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for inital embebdding and concat onto first set of filters from latent as well as image input in discrimanator\n",
    "class ClassEmbedding(layers.Layer):\n",
    "    def __init__(self, class_num, embedding_size, output_height,**kwargs):\n",
    "        super(ClassEmbedding, self).__init__(**kwargs)\n",
    "        self.class_num = class_num\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_height = output_height\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.embed = layers.Embedding(self.class_num, self.embedding_size,input_length=1)\n",
    "        self.dense1 = layers.Dense(self.output_height*self.output_height)\n",
    "        self.reshape = layers.Reshape((self.output_height,self.output_height,1))\n",
    "    def call(self, inputs):\n",
    "        x = self.embed(inputs)\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.reshape(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm(layers.Layer):\n",
    "    def __init__(self,class_num,embedding_size,training = True, **kwargs):\n",
    "        super(ConditionalBatchNorm, self).__init__(**kwargs)\n",
    "        self.training = training\n",
    "        self.class_num = class_num\n",
    "        self.decay = 0.9\n",
    "        self.epsilon = 1e-05\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert (type(input_shape) == list)\n",
    "        features, label = input_shape[0], input_shape[1]\n",
    "        self.channels = features[-1]\n",
    "        \n",
    "        zero_init = tf.keras.initializers.Constant(0.0)\n",
    "        one_init = tf.keras.initializers.Constant(1.0)\n",
    "        \n",
    "        self.test_mean = tf.Variable(name = \"pop_mean\", initial_value=zero_init(shape=(self.channels),dtype='float32'))\n",
    "        self.test_var = tf.Variable(name = \"pop_mean\", initial_value=one_init(shape=(self.channels),dtype='float32'))\n",
    "        \n",
    "        self.classEmbedding = layers.Embedding(self.class_num, self.embedding_size,input_length=1)\n",
    "        \n",
    "        self.betaLayer = layers.Dense(self.channels)\n",
    "        self.gammaLayer = layers.Dense(self.channels)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, label = inputs[0], inputs[1]\n",
    "        channels = x.shape[-1] \n",
    "        embedding = self.classEmbedding(label)\n",
    "        #print(embedding.shape)\n",
    "        \n",
    "        beta = self.betaLayer(embedding)\n",
    "        gamma = self.gammaLayer(embedding)\n",
    "        \n",
    "        beta = tf.reshape(beta, shape=[-1, 1, 1, self.channels])\n",
    "        gamma = tf.reshape(gamma, shape=[-1, 1, 1, self.channels])\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2])\n",
    "\n",
    "        if self.training:\n",
    "            batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2])\n",
    "            ema_mean = self.test_mean.assign(self.test_mean * self.decay + batch_mean * (1 - self.decay))\n",
    "            ema_var = self.test_var.assign(self.test_var * self.decay + batch_var * (1 - self.decay))\n",
    "            with tf.control_dependencies([ema_mean,ema_var]):\n",
    "                return tf.nn.batch_normalization(x, batch_mean, batch_var, beta, gamma, self.epsilon)\n",
    "        else:\n",
    "            return tf.nn.batch_normalization(x, self.test_mean, self.test_var, beta, gamma, self.epsilon)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbn = ConditionalBatchNorm(9,128)\n",
    "x = tf.random.uniform((4,4,4,3))\n",
    "cbn([x,tf.ones((4,1))])\n",
    "\n",
    "\n",
    "#cbn([x,tf.random.normal((4,3))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarMult(layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(ScalarMult, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.k = self.add_weight(\n",
    "            name='k',\n",
    "            shape=(),\n",
    "            initializer='zeros',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(ScalarMult, self).build(input_shape)\n",
    "    def call(self, inputs):\n",
    "        return tf.math.scalar_mul(self.k, inputs, name=None)\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        \n",
    "        self.key_weights = layers.Conv2D(channels // 8, 1, padding = 'same')\n",
    "        self.value_weights = layers.Conv2D(channels // 8, 1, padding = 'same')\n",
    "        self.query_weights = layers.Conv2D(channels, 1, padding = 'same')\n",
    "        \n",
    "        self.key_reshape = layers.Reshape(target_shape= (-1, input_shape[-1]//8))\n",
    "        self.value_reshape = layers.Reshape(target_shape = (-1,input_shape[-1]//8))\n",
    "    \n",
    "        self.query_reshape = layers.Reshape(target_shape = (-1, input_shape[-1]))\n",
    "        self.o_reshape = layers.Reshape(target_shape=input_shape)\n",
    "        \n",
    "        self.gamma = ScalarMult()\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        key = self.key_weights(inputs)\n",
    "        value = self.value_weights(inputs)\n",
    "        query = self.query_weights(inputs)\n",
    "        \n",
    "        \n",
    "        #key = tf.reshape(key, shape = [self.a[0], -1, self.a[-1]//8])\n",
    "        #value = tf.reshape(value, shape = [self.a[0], -1,self.a[-1]//8])\n",
    "        key = self.key_reshape(key)\n",
    "        value = self.value_reshape(value)\n",
    "                \n",
    "        scores = tf.matmul(key, value, transpose_b=True)\n",
    "        s_max = tf.nn.softmax(scores)\n",
    "        \n",
    "        #o = tf.matmul(s_max, tf.reshape(query, shape = [self.a[0],-1,self.a[-1]]))\n",
    "        q_reshape = self.query_reshape(query)\n",
    "        o = tf.matmul(s_max, q_reshape)\n",
    "        \n",
    "        #o = tf.reshape(o, shape = self.a)\n",
    "        o = self.o_reshape(o)\n",
    "        scaled_attention_map = self.gamma(o)\n",
    "\n",
    "        return inputs + scaled_attention_map\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfAttention = SelfAttention()\n",
    "test = tf.random.uniform((4,32,32,64))\n",
    "print(tf.shape(selfAttention(test)))\n",
    "\n",
    "\n",
    "def makeTest():\n",
    "    inputs = layers.Input(shape = (32,32,64))\n",
    "    layer = selfAttention\n",
    "    x = selfAttention(inputs)\n",
    "    return tf.keras.models.Model(inputs,x)\n",
    "\n",
    "x = makeTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeGenerator(latent_dim, initial_channels, class_num, embedding_size):\n",
    "    \n",
    "    in_latent = layers.Input(shape=(latent_dim,))\n",
    "    in_label = layers.Input(shape = (1,))\n",
    "#     \n",
    "    x = layers.Dense(4*4*initial_channels)(in_latent)\n",
    "    x = layers.Reshape((4,4,initial_channels))(x)\n",
    "    \n",
    "    x = ResBlockCN(class_num, embedding_size)([x,in_label])\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    #8x8\n",
    "    \n",
    "    x = ResBlockCN(class_num, embedding_size)([x,in_label])\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    #16x16\n",
    "    \n",
    "    x = ResBlockCN(class_num, embedding_size)([x,in_label])\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    #32x32\n",
    "    x = SelfAttention()(x)\n",
    "    \n",
    "    x = ResBlockCN(class_num, embedding_size)([x,in_label])\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    #64x64\n",
    "    print(x.shape)\n",
    "    \n",
    "    \n",
    "    x = ResBlockCN(class_num, embedding_size)([x,in_label])\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    \n",
    "    #128x128\n",
    "    #x = layers.Conv2D(3,(1,1), padding = \"same\", activation = 'sigmoid')(x)\n",
    "    model = tf.keras.models.Model([in_latent, in_label], x)\n",
    "    \n",
    "MakeGenerator(128,64,9,128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDiscriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
